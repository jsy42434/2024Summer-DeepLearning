{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhefwv+lSlMOXHQn0HxFMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsy42434/2024Summer-DeepLearning/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXmlTPksHEot",
        "outputId": "645755c8-a12e-46ff-93f9-8bc526eea806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 29 (delta 6), reused 19 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (29/29), 467.72 KiB | 8.50 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Epoch 1/5\n",
            "30/30 [==============================] - 1s 2ms/step - loss: 0.4336 - accuracy: 0.8447\n",
            "Epoch 2/5\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8489\n",
            "Epoch 3/5\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8511\n",
            "Epoch 4/5\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8511\n",
            "Epoch 5/5\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8511\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "Data_set = np.loadtxt(\"./data/ThoraricSurgery3.csv\",delimiter=\",\")\n",
        "X = Data_set[:,0:16]\n",
        "y = Data_set[:,16]\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Dense(30, input_dim=16, activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "history = model.fit(X,y,epochs=5,batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인디언 당뇨병 예측\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "df = pd.read_csv(\"./data/pima-indians-diabetes3.csv\")\n",
        "\n",
        "X = df.iloc[:,0:8]\n",
        "y = df.iloc[:,8]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu',name='Dense_1'))\n",
        "model.add(Dense(8, activation='relu', name='Dense_2'))\n",
        "model.add(Dense(4, activation='relu', name='Dense_3'))\n",
        "model.add(Dense(1, activation='sigmoid', name='Dense_4'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X,y,epochs=100,batch_size=5)\n"
      ],
      "metadata": {
        "id": "z_oLa6Q2StIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8f0dde-cfaa-4749-a3c8-450dcc8b53e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Dense_1 (Dense)             (None, 12)                108       \n",
            "                                                                 \n",
            " Dense_2 (Dense)             (None, 7)                 91        \n",
            "                                                                 \n",
            " Dense_3 (Dense)             (None, 4)                 32        \n",
            "                                                                 \n",
            " Dense_4 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 236 (944.00 Byte)\n",
            "Trainable params: 236 (944.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 1.3153 - accuracy: 0.6159\n",
            "Epoch 2/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.5677\n",
            "Epoch 3/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6574 - accuracy: 0.6497\n",
            "Epoch 4/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6460 - accuracy: 0.6471\n",
            "Epoch 5/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6562\n",
            "Epoch 6/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.6562\n",
            "Epoch 7/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6562\n",
            "Epoch 8/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6296 - accuracy: 0.6602\n",
            "Epoch 9/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.6549\n",
            "Epoch 10/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6138 - accuracy: 0.6602\n",
            "Epoch 11/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.6589\n",
            "Epoch 12/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.6104 - accuracy: 0.6706\n",
            "Epoch 13/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6189 - accuracy: 0.6589\n",
            "Epoch 14/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6088 - accuracy: 0.6719\n",
            "Epoch 15/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6095 - accuracy: 0.6693\n",
            "Epoch 16/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6067 - accuracy: 0.6719\n",
            "Epoch 17/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5968 - accuracy: 0.6797\n",
            "Epoch 18/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.6654\n",
            "Epoch 19/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5959 - accuracy: 0.6823\n",
            "Epoch 20/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.6758\n",
            "Epoch 21/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.6823\n",
            "Epoch 22/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.6719\n",
            "Epoch 23/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5912 - accuracy: 0.6771\n",
            "Epoch 24/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6810\n",
            "Epoch 25/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.6797\n",
            "Epoch 26/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5932 - accuracy: 0.6693\n",
            "Epoch 27/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.6732\n",
            "Epoch 28/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.6771\n",
            "Epoch 29/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5842 - accuracy: 0.6836\n",
            "Epoch 30/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.6810\n",
            "Epoch 31/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5859 - accuracy: 0.6966\n",
            "Epoch 32/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.6836\n",
            "Epoch 33/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5862 - accuracy: 0.6966\n",
            "Epoch 34/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6888\n",
            "Epoch 35/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.6953\n",
            "Epoch 36/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.6836\n",
            "Epoch 37/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.7057\n",
            "Epoch 38/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7109\n",
            "Epoch 39/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5714 - accuracy: 0.7044\n",
            "Epoch 40/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7057\n",
            "Epoch 41/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7018\n",
            "Epoch 42/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5673 - accuracy: 0.7070\n",
            "Epoch 43/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.6914\n",
            "Epoch 44/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.7044\n",
            "Epoch 45/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5672 - accuracy: 0.7253\n",
            "Epoch 46/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5619 - accuracy: 0.7240\n",
            "Epoch 47/100\n",
            "154/154 [==============================] - 1s 3ms/step - loss: 0.5563 - accuracy: 0.7253\n",
            "Epoch 48/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5573 - accuracy: 0.7201\n",
            "Epoch 49/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5549 - accuracy: 0.7188\n",
            "Epoch 50/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5559 - accuracy: 0.7279\n",
            "Epoch 51/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.7188\n",
            "Epoch 52/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5571 - accuracy: 0.6992\n",
            "Epoch 53/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7305\n",
            "Epoch 54/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5460 - accuracy: 0.7253\n",
            "Epoch 55/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7214\n",
            "Epoch 56/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7357\n",
            "Epoch 57/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7253\n",
            "Epoch 58/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5489 - accuracy: 0.7344\n",
            "Epoch 59/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.7279\n",
            "Epoch 60/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7370\n",
            "Epoch 61/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7383\n",
            "Epoch 62/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5444 - accuracy: 0.7318\n",
            "Epoch 63/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7266\n",
            "Epoch 64/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7122\n",
            "Epoch 65/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.7292\n",
            "Epoch 66/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5363 - accuracy: 0.7370\n",
            "Epoch 67/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7253\n",
            "Epoch 68/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7396\n",
            "Epoch 69/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7357\n",
            "Epoch 70/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7422\n",
            "Epoch 71/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7161\n",
            "Epoch 72/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.7279\n",
            "Epoch 73/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.7227\n",
            "Epoch 74/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.7266\n",
            "Epoch 75/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7188\n",
            "Epoch 76/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7448\n",
            "Epoch 77/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7240\n",
            "Epoch 78/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5442 - accuracy: 0.7344\n",
            "Epoch 79/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7318\n",
            "Epoch 80/100\n",
            "154/154 [==============================] - 0s 3ms/step - loss: 0.5286 - accuracy: 0.7292\n",
            "Epoch 81/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5258 - accuracy: 0.7461\n",
            "Epoch 82/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5194 - accuracy: 0.7383\n",
            "Epoch 83/100\n",
            "154/154 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7513\n",
            "Epoch 84/100\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5215 - accuracy: 0.7370\n",
            "Epoch 85/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7513\n",
            "Epoch 86/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7487\n",
            "Epoch 87/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7448\n",
            "Epoch 88/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7500\n",
            "Epoch 89/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7487\n",
            "Epoch 90/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7448\n",
            "Epoch 91/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7565\n",
            "Epoch 92/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7461\n",
            "Epoch 93/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7552\n",
            "Epoch 94/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7461\n",
            "Epoch 95/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7448\n",
            "Epoch 96/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7305\n",
            "Epoch 97/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7253\n",
            "Epoch 98/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7383\n",
            "Epoch 99/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7409\n",
            "Epoch 100/100\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#아이리스 꽃 분류\n",
        "#다중 분류를 해야 하는경우\n",
        "#원-핫 인코딩: 문자열을 숫자로 바꾸는 것\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "df = pd.read_csv(\"./data/iris3.csv\")\n",
        "\n",
        "X = df.iloc[:,0:4]\n",
        "y = df.iloc[:,4]\n",
        "\n",
        "y=pd.get_dummies(y)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=4, activation='relu',name='Dense_1'))\n",
        "model.add(Dense(8, activation='relu', name='Dense_2'))\n",
        "model.add(Dense(3, activation='softmax', name='Dense_3'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X,y,epochs=100,batch_size=5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6-gAfj4hhcH",
        "outputId": "fc01ed6f-514e-477d-c144-ee9b156d16c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Dense_1 (Dense)             (None, 12)                60        \n",
            "                                                                 \n",
            " Dense_2 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " Dense_3 (Dense)             (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 191 (764.00 Byte)\n",
            "Trainable params: 191 (764.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "30/30 [==============================] - 1s 2ms/step - loss: 1.5368 - accuracy: 0.3333\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1.2582 - accuracy: 0.3333\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1.1668 - accuracy: 0.3467\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1.1324 - accuracy: 0.3400\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0962 - accuracy: 0.3267\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1.0653 - accuracy: 0.4000\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1.0268 - accuracy: 0.5067\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.9882 - accuracy: 0.6400\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.9524 - accuracy: 0.6800\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.9030 - accuracy: 0.6800\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.8608 - accuracy: 0.8200\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.8171 - accuracy: 0.7200\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.7740 - accuracy: 0.7000\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.7345 - accuracy: 0.6933\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.6945 - accuracy: 0.7400\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.7800\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.8467\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.8733\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7733\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5486 - accuracy: 0.7667\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5240 - accuracy: 0.9067\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.8667\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.8800\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.9200\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8867\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.9800\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.9600\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.9467\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.9600\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.9533\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.9600\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.9533\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.3487 - accuracy: 0.9400\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.9733\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.9800\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.9733\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.9667\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.9733\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.9800\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.9800\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2359 - accuracy: 0.9733\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9800\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2209 - accuracy: 0.9800\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9800\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9733\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9733\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9800\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9800\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9600\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9800\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1712 - accuracy: 0.9800\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.9733\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9733\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9800\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9800\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9733\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9800\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9800\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9733\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1330 - accuracy: 0.9800\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9733\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.9667\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9800\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1272 - accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9800\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9467\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9733\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.1131 - accuracy: 0.9733\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9800\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9800\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.9800\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.1053 - accuracy: 0.9733\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9800\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0989 - accuracy: 0.9800\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9800\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9733\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.1059 - accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.9800\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9733\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0948 - accuracy: 0.9733\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9800\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0928 - accuracy: 0.9733\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0902 - accuracy: 0.9800\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9800\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9800\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9667\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0856 - accuracy: 0.9800\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0877 - accuracy: 0.9733\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9733\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9800\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9733\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.9733\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#광물분\n",
        "#테스트셋과 학습셋으로 나누어 과적합 방지\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "df = pd.read_csv(\"./data/sonar3.csv\")\n",
        "\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=60, activation='relu',name='Dense_1'))\n",
        "model.add(Dense(10, activation='relu', name='Dense_2'))\n",
        "model.add(Dense(1, activation='sigmoid', name='Dense_3'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train,y_train,epochs=200,batch_size=10)\n",
        "\n",
        "score = model.evaluate(X_test,y_test)\n",
        "print('Test accuracy',score[1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSlhbdlNlls-",
        "outputId": "068c6b93-1592-43a8-e3d2-4e86f95fd455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n",
            "Epoch 1/200\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 0.6898 - accuracy: 0.5625\n",
            "Epoch 2/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5972\n",
            "Epoch 3/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6786 - accuracy: 0.5972\n",
            "Epoch 4/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6762 - accuracy: 0.6042\n",
            "Epoch 5/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6726 - accuracy: 0.6111\n",
            "Epoch 6/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6685 - accuracy: 0.6042\n",
            "Epoch 7/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6181\n",
            "Epoch 8/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.6250\n",
            "Epoch 9/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.6528\n",
            "Epoch 10/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6430 - accuracy: 0.6597\n",
            "Epoch 11/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6370 - accuracy: 0.6806\n",
            "Epoch 12/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.6944\n",
            "Epoch 13/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6205 - accuracy: 0.7083\n",
            "Epoch 14/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.7014\n",
            "Epoch 15/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.7292\n",
            "Epoch 16/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5978 - accuracy: 0.7431\n",
            "Epoch 17/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.7431\n",
            "Epoch 18/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7639\n",
            "Epoch 19/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7569\n",
            "Epoch 20/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5671 - accuracy: 0.7847\n",
            "Epoch 21/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7778\n",
            "Epoch 22/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7917\n",
            "Epoch 23/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.7917\n",
            "Epoch 24/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7917\n",
            "Epoch 25/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5307 - accuracy: 0.8125\n",
            "Epoch 26/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7778\n",
            "Epoch 27/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.8264\n",
            "Epoch 28/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7986\n",
            "Epoch 29/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.8056\n",
            "Epoch 30/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8194\n",
            "Epoch 31/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.8264\n",
            "Epoch 32/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.8264\n",
            "Epoch 33/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4714 - accuracy: 0.8333\n",
            "Epoch 34/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.8194\n",
            "Epoch 35/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4611 - accuracy: 0.8264\n",
            "Epoch 36/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.8333\n",
            "Epoch 37/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4495 - accuracy: 0.8472\n",
            "Epoch 38/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.8542\n",
            "Epoch 39/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8472\n",
            "Epoch 40/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4275 - accuracy: 0.8542\n",
            "Epoch 41/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.8264\n",
            "Epoch 42/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8542\n",
            "Epoch 43/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8750\n",
            "Epoch 44/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8333\n",
            "Epoch 45/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.7986\n",
            "Epoch 46/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8611\n",
            "Epoch 47/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8472\n",
            "Epoch 48/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8681\n",
            "Epoch 49/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8611\n",
            "Epoch 50/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8264\n",
            "Epoch 51/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8333\n",
            "Epoch 52/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8542\n",
            "Epoch 53/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8681\n",
            "Epoch 54/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8542\n",
            "Epoch 55/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8542\n",
            "Epoch 56/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8194\n",
            "Epoch 57/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8681\n",
            "Epoch 58/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8681\n",
            "Epoch 59/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3558 - accuracy: 0.8542\n",
            "Epoch 60/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8750\n",
            "Epoch 61/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8681\n",
            "Epoch 62/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8750\n",
            "Epoch 63/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8403\n",
            "Epoch 64/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8958\n",
            "Epoch 65/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8681\n",
            "Epoch 66/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8750\n",
            "Epoch 67/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.8889\n",
            "Epoch 68/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8750\n",
            "Epoch 69/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8889\n",
            "Epoch 70/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8958\n",
            "Epoch 71/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8750\n",
            "Epoch 72/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8819\n",
            "Epoch 73/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8889\n",
            "Epoch 74/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3035 - accuracy: 0.8819\n",
            "Epoch 75/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8889\n",
            "Epoch 76/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.8819\n",
            "Epoch 77/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8889\n",
            "Epoch 78/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.8958\n",
            "Epoch 79/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2900 - accuracy: 0.8958\n",
            "Epoch 80/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.8958\n",
            "Epoch 81/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8958\n",
            "Epoch 82/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2849 - accuracy: 0.8958\n",
            "Epoch 83/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.9097\n",
            "Epoch 84/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.8958\n",
            "Epoch 85/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.9028\n",
            "Epoch 86/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8958\n",
            "Epoch 87/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2738 - accuracy: 0.9028\n",
            "Epoch 88/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8958\n",
            "Epoch 89/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8958\n",
            "Epoch 90/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2743 - accuracy: 0.9167\n",
            "Epoch 91/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.9167\n",
            "Epoch 92/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2629 - accuracy: 0.9097\n",
            "Epoch 93/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9097\n",
            "Epoch 94/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.9028\n",
            "Epoch 95/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.9167\n",
            "Epoch 96/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.9097\n",
            "Epoch 97/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9097\n",
            "Epoch 98/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.9167\n",
            "Epoch 99/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9236\n",
            "Epoch 100/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9167\n",
            "Epoch 101/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.9097\n",
            "Epoch 102/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.9167\n",
            "Epoch 103/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.9236\n",
            "Epoch 104/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2340 - accuracy: 0.9236\n",
            "Epoch 105/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9306\n",
            "Epoch 106/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9236\n",
            "Epoch 107/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.9236\n",
            "Epoch 108/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9236\n",
            "Epoch 109/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.9306\n",
            "Epoch 110/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2198 - accuracy: 0.9375\n",
            "Epoch 111/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9306\n",
            "Epoch 112/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9306\n",
            "Epoch 113/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2241 - accuracy: 0.9236\n",
            "Epoch 114/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2104 - accuracy: 0.9375\n",
            "Epoch 115/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9236\n",
            "Epoch 116/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9236\n",
            "Epoch 117/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9236\n",
            "Epoch 118/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.9306\n",
            "Epoch 119/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9444\n",
            "Epoch 120/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.9375\n",
            "Epoch 121/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.9375\n",
            "Epoch 122/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9375\n",
            "Epoch 123/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9444\n",
            "Epoch 124/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9444\n",
            "Epoch 125/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1885 - accuracy: 0.9375\n",
            "Epoch 126/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9375\n",
            "Epoch 127/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9375\n",
            "Epoch 128/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9375\n",
            "Epoch 129/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9375\n",
            "Epoch 130/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9514\n",
            "Epoch 131/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9375\n",
            "Epoch 132/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1782 - accuracy: 0.9375\n",
            "Epoch 133/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1734 - accuracy: 0.9375\n",
            "Epoch 134/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9444\n",
            "Epoch 135/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1751 - accuracy: 0.9514\n",
            "Epoch 136/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9444\n",
            "Epoch 137/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.9514\n",
            "Epoch 138/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9653\n",
            "Epoch 139/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9444\n",
            "Epoch 140/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1604 - accuracy: 0.9444\n",
            "Epoch 141/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.9583\n",
            "Epoch 142/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9444\n",
            "Epoch 143/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9722\n",
            "Epoch 144/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1559 - accuracy: 0.9514\n",
            "Epoch 145/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9583\n",
            "Epoch 146/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.9722\n",
            "Epoch 147/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1466 - accuracy: 0.9722\n",
            "Epoch 148/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.9514\n",
            "Epoch 149/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9792\n",
            "Epoch 150/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9792\n",
            "Epoch 151/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9583\n",
            "Epoch 152/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1397 - accuracy: 0.9722\n",
            "Epoch 153/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9722\n",
            "Epoch 154/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9514\n",
            "Epoch 155/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9861\n",
            "Epoch 156/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9792\n",
            "Epoch 157/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.9722\n",
            "Epoch 158/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.9861\n",
            "Epoch 159/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9792\n",
            "Epoch 160/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9861\n",
            "Epoch 161/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9861\n",
            "Epoch 162/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9861\n",
            "Epoch 163/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9861\n",
            "Epoch 164/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9861\n",
            "Epoch 165/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9792\n",
            "Epoch 166/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9861\n",
            "Epoch 167/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9861\n",
            "Epoch 168/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.9861\n",
            "Epoch 169/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9861\n",
            "Epoch 170/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9861\n",
            "Epoch 171/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9861\n",
            "Epoch 172/200\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9861\n",
            "Epoch 173/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9861\n",
            "Epoch 174/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.9861\n",
            "Epoch 175/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.9861\n",
            "Epoch 176/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9722\n",
            "Epoch 177/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9792\n",
            "Epoch 178/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1101 - accuracy: 0.9722\n",
            "Epoch 179/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9792\n",
            "Epoch 180/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1035 - accuracy: 0.9861\n",
            "Epoch 181/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9861\n",
            "Epoch 182/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.9861\n",
            "Epoch 183/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9861\n",
            "Epoch 184/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9861\n",
            "Epoch 185/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0941 - accuracy: 0.9861\n",
            "Epoch 186/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9861\n",
            "Epoch 187/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9931\n",
            "Epoch 188/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.9861\n",
            "Epoch 189/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9861\n",
            "Epoch 190/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9861\n",
            "Epoch 191/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9583\n",
            "Epoch 192/200\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0987 - accuracy: 0.9861\n",
            "Epoch 193/200\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0876 - accuracy: 0.9861\n",
            "Epoch 194/200\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0844 - accuracy: 0.9861\n",
            "Epoch 195/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.9861\n",
            "Epoch 196/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.9861\n",
            "Epoch 197/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.9861\n",
            "Epoch 198/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9861\n",
            "Epoch 199/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9861\n",
            "Epoch 200/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0769 - accuracy: 0.9861\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4638 - accuracy: 0.8254\n",
            "Test accuracy 0.8253968358039856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#k곂 교차검증 06.28\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "df = pd.read_csv(\"./data/sonar3.csv\")\n",
        "\n",
        "X = df.iloc[:,0:60]\n",
        "y = df.iloc[:,60]\n",
        "\n",
        "k=5\n",
        "\n",
        "kfold = KFold(n_splits=k, shuffle=True)\n",
        "acc_score = []\n",
        "\n",
        "def model_fn():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(24, input_dim=60, activation='relu',name='Dense_1'))\n",
        "  model.add(Dense(10, activation='relu', name='Dense_2'))\n",
        "  model.add(Dense(1, activation='sigmoid', name='Dense_3'))\n",
        "  return model\n",
        "\n",
        "for train_index, test_index in kfold.split(X):\n",
        "  X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  model = model_fn()\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(X_train,y_train,epochs=200,batch_size=10)\n",
        "  accuracy = model.evaluate(X_test,y_test)[1]\n",
        "  acc_score.append(accuracy)\n",
        "\n",
        "avg_acc_score = sum(acc_score)/k\n",
        "print('정확도: ',acc_score)\n",
        "print('정확도 평균',avg_acc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_sr9IaQnUeQ",
        "outputId": "449fc8cb-d8e2-44d7-da47-837d7f7ed564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5758\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6636 - accuracy: 0.5758\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.5879\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.6182\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.6848\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6142 - accuracy: 0.7030\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.7152\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7212\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7333\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5422 - accuracy: 0.7394\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7333\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7333\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.5038 - accuracy: 0.7939\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7697\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4725 - accuracy: 0.7879\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4716 - accuracy: 0.7697\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.8121\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.8061\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8061\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8182\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8121\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8182\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8364\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8242\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8303\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8485\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8061\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3622 - accuracy: 0.8667\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8424\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8667\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8606\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8606\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8485\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8727\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3111 - accuracy: 0.8788\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8667\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.8970\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8909\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8909\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.9030\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2841 - accuracy: 0.8667\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.9091\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9212\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9212\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8970\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.8970\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.9212\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9091\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.9273\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.9273\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9273\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.9273\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2285 - accuracy: 0.9212\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9515\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2124 - accuracy: 0.9515\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9212\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2017 - accuracy: 0.9394\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9636\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9212\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9455\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9333\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1967 - accuracy: 0.9455\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.9455\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1783 - accuracy: 0.9697\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1738 - accuracy: 0.9576\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9636\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1655 - accuracy: 0.9576\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9576\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1616 - accuracy: 0.9697\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1547 - accuracy: 0.9818\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1625 - accuracy: 0.9576\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1498 - accuracy: 0.9515\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9697\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1436 - accuracy: 0.9818\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9636\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.9818\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9758\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1321 - accuracy: 0.9758\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1259 - accuracy: 0.9818\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1248 - accuracy: 0.9818\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.9697\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9758\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9636\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9758\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9879\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9758\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1029 - accuracy: 0.9879\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1018 - accuracy: 0.9818\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9818\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9758\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9879\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.9879\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9818\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9879\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9879\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9879\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9879\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9939\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.9939\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9879\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9939\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9939\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9939\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9939\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9879\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9939\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9939\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9939\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9939\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9939\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0514 - accuracy: 0.9939\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9939\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x787b35f66ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5734 - accuracy: 0.8333\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 1s 2ms/step - loss: 0.6967 - accuracy: 0.4848\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.6182\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6781 - accuracy: 0.6788\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.6727\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6634 - accuracy: 0.6970\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.7152\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.7030\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6276 - accuracy: 0.7212\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6125 - accuracy: 0.7273\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5996 - accuracy: 0.7273\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5855 - accuracy: 0.7576\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5653 - accuracy: 0.8000\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.8000\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5280 - accuracy: 0.8061\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.5147 - accuracy: 0.7939\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4984 - accuracy: 0.8242\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4789 - accuracy: 0.8303\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.4653 - accuracy: 0.8182\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4531 - accuracy: 0.8485\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.8364\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8303\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4240 - accuracy: 0.8303\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.4045 - accuracy: 0.8424\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.4103 - accuracy: 0.8242\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.3917 - accuracy: 0.8364\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3863 - accuracy: 0.8364\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3787 - accuracy: 0.8364\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8606\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8364\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.8606\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.3495 - accuracy: 0.8545\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.3491 - accuracy: 0.8303\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.8848\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8667\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3351 - accuracy: 0.8485\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3241 - accuracy: 0.8727\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8788\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8667\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3092 - accuracy: 0.8909\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3014 - accuracy: 0.8848\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2987 - accuracy: 0.8848\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.9030\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8970\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2876 - accuracy: 0.8909\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2906 - accuracy: 0.8848\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2831 - accuracy: 0.9152\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2728 - accuracy: 0.9152\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2673 - accuracy: 0.9091\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.2638 - accuracy: 0.9030\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2715 - accuracy: 0.9030\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2597 - accuracy: 0.9152\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2562 - accuracy: 0.8970\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2526 - accuracy: 0.9091\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.9091\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9212\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9091\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2473 - accuracy: 0.9091\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2329 - accuracy: 0.9152\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2257 - accuracy: 0.9212\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2347 - accuracy: 0.9212\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.9212\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 0.9273\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9273\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9333\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9273\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2009 - accuracy: 0.9333\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9273\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9273\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2008 - accuracy: 0.9394\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9273\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1956 - accuracy: 0.9394\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1992 - accuracy: 0.9394\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1977 - accuracy: 0.9333\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9455\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.9394\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9576\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9576\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9576\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9333\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.9636\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1652 - accuracy: 0.9515\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1614 - accuracy: 0.9636\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.9576\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9576\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9636\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1529 - accuracy: 0.9697\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9515\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9394\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.9636\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9697\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.9455\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9758\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1515 - accuracy: 0.9455\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9515\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9576\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.9636\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9697\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9697\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9636\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9697\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9697\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9758\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9697\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9818\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9697\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9697\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9758\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.9758\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9758\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9697\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9758\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1014 - accuracy: 0.9758\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9818\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0998 - accuracy: 0.9758\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0999 - accuracy: 0.9818\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0954 - accuracy: 0.9818\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.9758\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9818\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9879\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9879\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9879\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.9879\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9879\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9879\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9879\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9818\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1016 - accuracy: 0.9576\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9939\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0803 - accuracy: 0.9879\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9939\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9939\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9939\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9939\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9879\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.9939\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9939\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.9939\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9939\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9879\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9939\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0652 - accuracy: 0.9939\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9879\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9939\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9939\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9939\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9879\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9939\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9879\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9939\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9939\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9939\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9939\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9939\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9939\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9939\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9939\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9939\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0498 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9939\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9939\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9939\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0632 - accuracy: 0.9939\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9939\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9939\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.9939\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.9939\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9939\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9939\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.9939\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9939\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9939\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9939\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.9939\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5901 - accuracy: 0.7857\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.5060\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6600 - accuracy: 0.6988\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6468 - accuracy: 0.7349\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.7530\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.7530\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6122 - accuracy: 0.7470\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.7470\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.7530\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7651\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7590\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.7711\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5388 - accuracy: 0.7952\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.8012\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7892\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.8012\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7711\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4834 - accuracy: 0.8012\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.8012\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.8133\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.8193\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8193\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8253\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8434\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8373\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8373\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4105 - accuracy: 0.8313\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8494\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.8494\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3783 - accuracy: 0.8554\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8554\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8735\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8855\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3525 - accuracy: 0.8675\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3467 - accuracy: 0.8795\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.8855\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8735\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8855\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8675\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8855\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8855\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8795\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3111 - accuracy: 0.8976\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8916\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8976\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8976\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8916\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8795\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8916\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.9036\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8976\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2694 - accuracy: 0.8916\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8976\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.9036\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8916\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8976\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2527 - accuracy: 0.9157\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2485 - accuracy: 0.9096\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9217\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2346 - accuracy: 0.9036\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2344 - accuracy: 0.9217\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2333 - accuracy: 0.9036\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2238 - accuracy: 0.9217\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2175 - accuracy: 0.9217\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2108 - accuracy: 0.9277\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.2140 - accuracy: 0.9277\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2123 - accuracy: 0.9277\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9096\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9096\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9277\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1936 - accuracy: 0.9277\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1914 - accuracy: 0.9217\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1994 - accuracy: 0.9337\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9277\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1850 - accuracy: 0.9277\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9458\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9337\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9518\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1715 - accuracy: 0.9398\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1698 - accuracy: 0.9398\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1681 - accuracy: 0.9337\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9458\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9398\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9458\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9518\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1503 - accuracy: 0.9458\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9518\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1411 - accuracy: 0.9458\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9458\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9458\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9518\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9458\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1384 - accuracy: 0.9518\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1433 - accuracy: 0.9518\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9578\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9458\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9639\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9518\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9699\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9639\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1276 - accuracy: 0.9518\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9699\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.1081 - accuracy: 0.9699\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9578\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9699\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1042 - accuracy: 0.9759\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9759\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9639\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.9699\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9819\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9759\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9699\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9639\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9819\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0989 - accuracy: 0.9699\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0915 - accuracy: 0.9880\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9759\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9819\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9819\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9759\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9819\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9819\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9819\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9759\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9759\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9819\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9759\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9819\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9819\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 14ms/step - loss: 0.0720 - accuracy: 0.9819\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 0.0685 - accuracy: 0.9880\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.9819\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0662 - accuracy: 0.9819\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0652 - accuracy: 0.9819\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9819\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9819\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9880\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9819\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0691 - accuracy: 0.9699\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9819\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9819\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9880\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 13ms/step - loss: 0.0716 - accuracy: 0.9759\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9699\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9880\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9819\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9880\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9880\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9880\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9880\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9940\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9880\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9880\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9880\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 9ms/step - loss: 0.0470 - accuracy: 0.9940\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 15ms/step - loss: 0.0442 - accuracy: 0.9940\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9880\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9880\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0441 - accuracy: 0.9940\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9940\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9940\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0451 - accuracy: 0.9880\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9940\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9940\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9940\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9940\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9940\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9880\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9940\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.9940\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.9940\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 0.9940\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.9940\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9940\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9940\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9940\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.9940\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9940\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9940\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9940\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0162 - accuracy: 0.7561\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 1s 2ms/step - loss: 0.6928 - accuracy: 0.5120\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6717 - accuracy: 0.6265\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6553 - accuracy: 0.6145\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.6325\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6245 - accuracy: 0.6627\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6123 - accuracy: 0.6807\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.7108\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.7530\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5719 - accuracy: 0.7349\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7349\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7410\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.7831\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.5138 - accuracy: 0.8253\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.5026 - accuracy: 0.7771\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7952\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4864 - accuracy: 0.8072\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4698 - accuracy: 0.8193\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.8193\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.8373\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.8434\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.8554\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8133\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8193\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4193 - accuracy: 0.8253\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8554\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8554\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8313\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8554\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.4033 - accuracy: 0.8373\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3883 - accuracy: 0.8434\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3836 - accuracy: 0.8373\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.3720 - accuracy: 0.8554\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3719 - accuracy: 0.8434\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8735\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8494\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8735\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8795\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8795\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8735\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8855\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8795\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8735\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8855\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8795\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3097 - accuracy: 0.8795\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8916\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3000 - accuracy: 0.8855\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2996 - accuracy: 0.8795\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2971 - accuracy: 0.8916\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8976\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8976\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2828 - accuracy: 0.8976\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.9157\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8855\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8614\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.8976\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8976\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8976\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.8916\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2555 - accuracy: 0.9036\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.9096\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8916\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2481 - accuracy: 0.9217\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.9096\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2379 - accuracy: 0.9096\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.9036\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.9217\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2353 - accuracy: 0.9096\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9217\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.9096\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9277\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9277\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9337\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9217\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9337\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9337\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9277\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9157\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9398\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1992 - accuracy: 0.9398\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9398\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9337\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9398\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9398\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9458\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.9518\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.9398\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9578\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9458\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.9518\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1728 - accuracy: 0.9458\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9458\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9518\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9398\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9578\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1567 - accuracy: 0.9578\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9578\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9578\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9458\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9518\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9518\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9578\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1557 - accuracy: 0.9458\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9578\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9639\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9639\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9639\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.9639\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9578\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9639\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9639\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9639\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9639\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9639\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1266 - accuracy: 0.9639\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9639\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.9639\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.9518\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9639\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9639\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.9639\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 0.9639\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9639\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9699\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.9639\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1068 - accuracy: 0.9759\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1087 - accuracy: 0.9699\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9699\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1079 - accuracy: 0.9639\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.9699\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9699\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.9699\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9699\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9699\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9699\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.9819\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9699\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9759\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0943 - accuracy: 0.9759\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9880\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9699\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9759\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9819\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0908 - accuracy: 0.9880\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9759\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0850 - accuracy: 0.9819\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.9819\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9880\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9880\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9759\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9880\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9880\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9819\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9880\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 0.9880\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9880\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9880\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9819\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.9880\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9880\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9880\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9880\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.9880\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0660 - accuracy: 0.9880\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0627 - accuracy: 0.9880\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9880\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9880\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9880\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9880\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9880\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9880\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9880\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9880\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9880\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9880\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9880\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9880\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9880\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9880\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0503 - accuracy: 0.9880\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9880\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9880\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9880\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.9880\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9880\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 0.9880\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9880\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9880\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9880\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9880\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9880\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9880\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 0.9880\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9880\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0447 - accuracy: 0.9880\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0422 - accuracy: 0.9880\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9880\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 0.9880\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0412 - accuracy: 0.9880\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9880\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2300 - accuracy: 0.9268\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 1s 2ms/step - loss: 0.6840 - accuracy: 0.5723\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6648 - accuracy: 0.6386\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.6747\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6366 - accuracy: 0.6988\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.6243 - accuracy: 0.7349\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.6090 - accuracy: 0.7470\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.6011 - accuracy: 0.7349\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.5836 - accuracy: 0.7229\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.7952\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5511 - accuracy: 0.7831\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5390 - accuracy: 0.8012\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5266 - accuracy: 0.7651\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.5145 - accuracy: 0.7952\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.5094 - accuracy: 0.7590\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7892\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.8012\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.8072\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.7892\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.7952\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4402 - accuracy: 0.8012\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.7952\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8012\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4132 - accuracy: 0.8133\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.4129 - accuracy: 0.8072\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8133\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8193\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8193\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8253\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3760 - accuracy: 0.8313\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8193\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8434\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.8253\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8434\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8253\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8494\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8554\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8554\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3519 - accuracy: 0.8133\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3546 - accuracy: 0.8253\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8614\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8133\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8373\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8494\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8675\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3321 - accuracy: 0.8373\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8614\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.8313\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 19ms/step - loss: 0.3136 - accuracy: 0.8735\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8855\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.8795\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.8795\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8855\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2933 - accuracy: 0.8916\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8855\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2865 - accuracy: 0.8916\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2935 - accuracy: 0.8735\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.8916\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8675\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2884 - accuracy: 0.8614\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.8916\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8916\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.8795\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2619 - accuracy: 0.9036\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.9036\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2575 - accuracy: 0.9096\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2541 - accuracy: 0.9036\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2501 - accuracy: 0.9157\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2481 - accuracy: 0.9096\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2435 - accuracy: 0.9096\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.2511 - accuracy: 0.8795\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.8976\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9337\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9036\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9277\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9277\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 1s 2ms/step - loss: 0.2469 - accuracy: 0.8976\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9217\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9277\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2167 - accuracy: 0.9217\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9217\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2102 - accuracy: 0.9277\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9337\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9277\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.9217\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9277\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9157\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9398\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9337\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1922 - accuracy: 0.9458\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9458\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9578\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9458\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9458\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9518\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9458\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9518\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9458\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1740 - accuracy: 0.9458\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9458\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9518\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9578\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1622 - accuracy: 0.9518\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9277\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.8735\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9518\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1542 - accuracy: 0.9639\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9578\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 0.9639\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1531 - accuracy: 0.9518\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.9458\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9578\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1463 - accuracy: 0.9578\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9578\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9639\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.9639\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9578\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1338 - accuracy: 0.9639\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9518\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1424 - accuracy: 0.9639\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9578\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1209 - accuracy: 0.9578\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9578\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9639\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9639\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.1149 - accuracy: 0.9639\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.9639\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9699\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1211 - accuracy: 0.9639\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9759\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1135 - accuracy: 0.9639\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.9639\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9699\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9699\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9819\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9819\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.1094 - accuracy: 0.9578\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9819\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9759\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.1036 - accuracy: 0.9819\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9759\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.9759\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9699\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0993 - accuracy: 0.9759\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9880\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0890 - accuracy: 0.9819\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.9819\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9880\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9759\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.9880\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9819\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9819\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9880\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9819\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.9940\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9940\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9819\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9880\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.9880\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9880\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9940\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.9940\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0588 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0582 - accuracy: 0.9940\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0554 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9940\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9880\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9940\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0440 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 6ms/step - loss: 0.0416 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9940\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6296 - accuracy: 0.8780\n",
            "정확도:  [0.8333333134651184, 0.7857142686843872, 0.7560975551605225, 0.9268292784690857, 0.8780487775802612]\n",
            "정확도 평균 0.836004638671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 검증셋과 학습 자동중단\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "modelpath = './data/model/Ch14-4-bestmodel.hdf5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verboss=0, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train,y_train,epochs=2000,batch_size=500,validation_split=0.25,verbose=1,callbacks=[early_stopping_callback,checkpointer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL48UlgrvDwH",
        "outputId": "76d919db-ecdb-441b-d3a8-adcf18463ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.0514 - accuracy: 0.9907 - val_loss: 0.0590 - val_accuracy: 0.9722\n",
            "Epoch 2/2000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0514 - accuracy: 0.9907 - val_loss: 0.0591 - val_accuracy: 0.9722\n",
            "Epoch 3/2000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0513 - accuracy: 0.9907 - val_loss: 0.0592 - val_accuracy: 0.9722\n",
            "Epoch 4/2000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0512 - accuracy: 0.9907 - val_loss: 0.0593 - val_accuracy: 0.9722\n",
            "Epoch 5/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9907"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 67ms/step - loss: 0.0511 - accuracy: 0.9907 - val_loss: 0.0594 - val_accuracy: 0.9722\n",
            "Epoch 6/2000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0511 - accuracy: 0.9907 - val_loss: 0.0595 - val_accuracy: 0.9722\n",
            "Epoch 7/2000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0510 - accuracy: 0.9907 - val_loss: 0.0596 - val_accuracy: 0.9722\n",
            "Epoch 8/2000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0509 - accuracy: 0.9907 - val_loss: 0.0596 - val_accuracy: 0.9722\n",
            "Epoch 9/2000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0509 - accuracy: 0.9907 - val_loss: 0.0597 - val_accuracy: 0.9722\n",
            "Epoch 10/2000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0508 - accuracy: 0.9907 - val_loss: 0.0598 - val_accuracy: 0.9722\n",
            "Epoch 11/2000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0507 - accuracy: 0.9907 - val_loss: 0.0599 - val_accuracy: 0.9722\n",
            "Epoch 12/2000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0506 - accuracy: 0.9907 - val_loss: 0.0600 - val_accuracy: 0.9722\n",
            "Epoch 13/2000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0506 - accuracy: 0.9907 - val_loss: 0.0601 - val_accuracy: 0.9722\n",
            "Epoch 14/2000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0505 - accuracy: 0.9907 - val_loss: 0.0602 - val_accuracy: 0.9722\n",
            "Epoch 15/2000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0504 - accuracy: 0.9907 - val_loss: 0.0603 - val_accuracy: 0.9722\n",
            "Epoch 16/2000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0503 - accuracy: 0.9907 - val_loss: 0.0604 - val_accuracy: 0.9722\n",
            "Epoch 17/2000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0503 - accuracy: 0.9907 - val_loss: 0.0605 - val_accuracy: 0.9722\n",
            "Epoch 18/2000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0502 - accuracy: 0.9907 - val_loss: 0.0606 - val_accuracy: 0.9722\n",
            "Epoch 19/2000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0501 - accuracy: 0.9907 - val_loss: 0.0607 - val_accuracy: 0.9722\n",
            "Epoch 20/2000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0501 - accuracy: 0.9907 - val_loss: 0.0607 - val_accuracy: 0.9722\n",
            "Epoch 21/2000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0500 - accuracy: 0.9907 - val_loss: 0.0608 - val_accuracy: 0.9722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#주택가격 예측하기\n",
        "#데이터 전처리 하는 방법\n",
        "#결측치 채우기, 속성별 관련도 추출하기\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "df = pd.read_csv(\"./data/house_train.csv\")\n",
        "\n",
        "df = pd.get_dummies(df)\n",
        "\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "df_corr = df.corr()\n",
        "\n",
        "df_corr_sort = df_corr.sort_values(by='SalePrice',ascending=False)\n",
        "\n",
        "cols_train = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF']\n",
        "\n",
        "X_train_pre = df[cols_train]\n",
        "\n",
        "y = df['SalePrice'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_pre,y,test_size=0.2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(30, activation='relu'))\n",
        "model.add(Dense(40, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "modelpath = './data/model/Ch15-house.hdf5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verboss=0, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train,y_train,epochs=2000,batch_size=32,validation_split=0.25,callbacks=[early_stopping_callback,checkpointer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L723y-LAy1Yr",
        "outputId": "370c10db-d74c-4b13-9468-c384449a34dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 10)                60        \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 30)                330       \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 40)                1240      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 41        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1671 (6.53 KB)\n",
            "Trainable params: 1671 (6.53 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/2000\n",
            "28/28 [==============================] - 2s 20ms/step - loss: 38959902720.0000 - val_loss: 40539549696.0000\n",
            "Epoch 2/2000\n",
            "16/28 [================>.............] - ETA: 0s - loss: 38862045184.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 0s 8ms/step - loss: 38833102848.0000 - val_loss: 40403886080.0000\n",
            "Epoch 3/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 38676615168.0000 - val_loss: 40187944960.0000\n",
            "Epoch 4/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 38410379264.0000 - val_loss: 39810879488.0000\n",
            "Epoch 5/2000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 37941678080.0000 - val_loss: 39175016448.0000\n",
            "Epoch 6/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 37148217344.0000 - val_loss: 38050062336.0000\n",
            "Epoch 7/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 35675426816.0000 - val_loss: 35747291136.0000\n",
            "Epoch 8/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 32635793408.0000 - val_loss: 31334316032.0000\n",
            "Epoch 9/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 27384127488.0000 - val_loss: 24331538432.0000\n",
            "Epoch 10/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 19745064960.0000 - val_loss: 15177110528.0000\n",
            "Epoch 11/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 10988253184.0000 - val_loss: 6839079936.0000\n",
            "Epoch 12/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 4684476928.0000 - val_loss: 2585359872.0000\n",
            "Epoch 13/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 2511598848.0000 - val_loss: 1859429376.0000\n",
            "Epoch 14/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2254528256.0000 - val_loss: 1849852160.0000\n",
            "Epoch 15/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2244810240.0000 - val_loss: 1844862976.0000\n",
            "Epoch 16/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2243005952.0000 - val_loss: 1838949760.0000\n",
            "Epoch 17/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2235667456.0000 - val_loss: 1834977280.0000\n",
            "Epoch 18/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2230619904.0000 - val_loss: 1830517376.0000\n",
            "Epoch 19/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2229559808.0000 - val_loss: 1825582720.0000\n",
            "Epoch 20/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2218859008.0000 - val_loss: 1821080704.0000\n",
            "Epoch 21/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2222836480.0000 - val_loss: 1815642368.0000\n",
            "Epoch 22/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2210025984.0000 - val_loss: 1811418880.0000\n",
            "Epoch 23/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2213312000.0000 - val_loss: 1808203776.0000\n",
            "Epoch 24/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2200501248.0000 - val_loss: 1803535616.0000\n",
            "Epoch 25/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2197366528.0000 - val_loss: 1798683008.0000\n",
            "Epoch 26/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2196010752.0000 - val_loss: 1794769664.0000\n",
            "Epoch 27/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2188213504.0000 - val_loss: 1795163904.0000\n",
            "Epoch 28/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2184349952.0000 - val_loss: 1787251840.0000\n",
            "Epoch 29/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2177801728.0000 - val_loss: 1783038720.0000\n",
            "Epoch 30/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2177744128.0000 - val_loss: 1778974336.0000\n",
            "Epoch 31/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2173300224.0000 - val_loss: 1775258752.0000\n",
            "Epoch 32/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2164758784.0000 - val_loss: 1774084608.0000\n",
            "Epoch 33/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2161820416.0000 - val_loss: 1768759936.0000\n",
            "Epoch 34/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2163321856.0000 - val_loss: 1766177408.0000\n",
            "Epoch 35/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2156686336.0000 - val_loss: 1764011136.0000\n",
            "Epoch 36/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2159212544.0000 - val_loss: 1760011904.0000\n",
            "Epoch 37/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2152013824.0000 - val_loss: 1756131456.0000\n",
            "Epoch 38/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2157944064.0000 - val_loss: 1753873408.0000\n",
            "Epoch 39/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2143175808.0000 - val_loss: 1750739200.0000\n",
            "Epoch 40/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2142828032.0000 - val_loss: 1748611072.0000\n",
            "Epoch 41/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2136610176.0000 - val_loss: 1745726464.0000\n",
            "Epoch 42/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2138312064.0000 - val_loss: 1742778496.0000\n",
            "Epoch 43/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2130361984.0000 - val_loss: 1740719360.0000\n",
            "Epoch 44/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2128779520.0000 - val_loss: 1739459328.0000\n",
            "Epoch 45/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2130421376.0000 - val_loss: 1736046848.0000\n",
            "Epoch 46/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2126130304.0000 - val_loss: 1734124544.0000\n",
            "Epoch 47/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2129437696.0000 - val_loss: 1732073984.0000\n",
            "Epoch 48/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2124852096.0000 - val_loss: 1729710336.0000\n",
            "Epoch 49/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2116816000.0000 - val_loss: 1730590208.0000\n",
            "Epoch 50/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2117031680.0000 - val_loss: 1725741696.0000\n",
            "Epoch 51/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2112765312.0000 - val_loss: 1723715712.0000\n",
            "Epoch 52/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2110085888.0000 - val_loss: 1722722432.0000\n",
            "Epoch 53/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2114160384.0000 - val_loss: 1720589696.0000\n",
            "Epoch 54/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2109873664.0000 - val_loss: 1718877952.0000\n",
            "Epoch 55/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2111793664.0000 - val_loss: 1717220608.0000\n",
            "Epoch 56/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2118013824.0000 - val_loss: 1715887616.0000\n",
            "Epoch 57/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2101570432.0000 - val_loss: 1714150144.0000\n",
            "Epoch 58/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2103547136.0000 - val_loss: 1714353280.0000\n",
            "Epoch 59/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2099958784.0000 - val_loss: 1712694144.0000\n",
            "Epoch 60/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2106889984.0000 - val_loss: 1710542464.0000\n",
            "Epoch 61/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2111480192.0000 - val_loss: 1708632576.0000\n",
            "Epoch 62/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2093002112.0000 - val_loss: 1709634432.0000\n",
            "Epoch 63/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2092321920.0000 - val_loss: 1706224128.0000\n",
            "Epoch 64/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2095856896.0000 - val_loss: 1705343616.0000\n",
            "Epoch 65/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2087615488.0000 - val_loss: 1705163392.0000\n",
            "Epoch 66/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2087875072.0000 - val_loss: 1702866688.0000\n",
            "Epoch 67/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2084732032.0000 - val_loss: 1702959104.0000\n",
            "Epoch 68/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2084972800.0000 - val_loss: 1700402688.0000\n",
            "Epoch 69/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2085555584.0000 - val_loss: 1700504064.0000\n",
            "Epoch 70/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2083472128.0000 - val_loss: 1699002368.0000\n",
            "Epoch 71/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2079707904.0000 - val_loss: 1698374400.0000\n",
            "Epoch 72/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2085369856.0000 - val_loss: 1698191360.0000\n",
            "Epoch 73/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2085641344.0000 - val_loss: 1698084224.0000\n",
            "Epoch 74/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2080506112.0000 - val_loss: 1699339392.0000\n",
            "Epoch 75/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2081738496.0000 - val_loss: 1696972800.0000\n",
            "Epoch 76/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2079440768.0000 - val_loss: 1695940096.0000\n",
            "Epoch 77/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2079592704.0000 - val_loss: 1696366208.0000\n",
            "Epoch 78/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2091068928.0000 - val_loss: 1694243456.0000\n",
            "Epoch 79/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2068496128.0000 - val_loss: 1703528960.0000\n",
            "Epoch 80/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2080459136.0000 - val_loss: 1691943936.0000\n",
            "Epoch 81/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2078035200.0000 - val_loss: 1691329408.0000\n",
            "Epoch 82/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2077256576.0000 - val_loss: 1694055808.0000\n",
            "Epoch 83/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2072985728.0000 - val_loss: 1692893568.0000\n",
            "Epoch 84/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2069055360.0000 - val_loss: 1690103552.0000\n",
            "Epoch 85/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2071423488.0000 - val_loss: 1689741312.0000\n",
            "Epoch 86/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2071337984.0000 - val_loss: 1688958208.0000\n",
            "Epoch 87/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2074750464.0000 - val_loss: 1689498496.0000\n",
            "Epoch 88/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2077131904.0000 - val_loss: 1689196544.0000\n",
            "Epoch 89/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2079519872.0000 - val_loss: 1687664384.0000\n",
            "Epoch 90/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2071466240.0000 - val_loss: 1689702016.0000\n",
            "Epoch 91/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2087159808.0000 - val_loss: 1689158784.0000\n",
            "Epoch 92/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2066907136.0000 - val_loss: 1686731392.0000\n",
            "Epoch 93/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2069858176.0000 - val_loss: 1686111488.0000\n",
            "Epoch 94/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2070195712.0000 - val_loss: 1685751296.0000\n",
            "Epoch 95/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2075283456.0000 - val_loss: 1685517184.0000\n",
            "Epoch 96/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2069122432.0000 - val_loss: 1685730816.0000\n",
            "Epoch 97/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2070854912.0000 - val_loss: 1684768768.0000\n",
            "Epoch 98/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2063938304.0000 - val_loss: 1684836096.0000\n",
            "Epoch 99/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2067532928.0000 - val_loss: 1685092096.0000\n",
            "Epoch 100/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2065046272.0000 - val_loss: 1684057728.0000\n",
            "Epoch 101/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2065764480.0000 - val_loss: 1687138688.0000\n",
            "Epoch 102/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2067770368.0000 - val_loss: 1685537408.0000\n",
            "Epoch 103/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2066646144.0000 - val_loss: 1683583104.0000\n",
            "Epoch 104/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2070654720.0000 - val_loss: 1684987904.0000\n",
            "Epoch 105/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2069176704.0000 - val_loss: 1683495296.0000\n",
            "Epoch 106/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2061770880.0000 - val_loss: 1683747456.0000\n",
            "Epoch 107/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2063645056.0000 - val_loss: 1683716608.0000\n",
            "Epoch 108/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2062865920.0000 - val_loss: 1683808000.0000\n",
            "Epoch 109/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2067401344.0000 - val_loss: 1682276224.0000\n",
            "Epoch 110/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2070709120.0000 - val_loss: 1682230784.0000\n",
            "Epoch 111/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2065593216.0000 - val_loss: 1683165568.0000\n",
            "Epoch 112/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2065038208.0000 - val_loss: 1682290560.0000\n",
            "Epoch 113/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2062627584.0000 - val_loss: 1682800000.0000\n",
            "Epoch 114/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2059996544.0000 - val_loss: 1682985728.0000\n",
            "Epoch 115/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2058501376.0000 - val_loss: 1682011648.0000\n",
            "Epoch 116/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2059124992.0000 - val_loss: 1681363456.0000\n",
            "Epoch 117/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2067854208.0000 - val_loss: 1680750080.0000\n",
            "Epoch 118/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2060892544.0000 - val_loss: 1684025856.0000\n",
            "Epoch 119/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2059652352.0000 - val_loss: 1681209088.0000\n",
            "Epoch 120/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2064920064.0000 - val_loss: 1683836288.0000\n",
            "Epoch 121/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2063773696.0000 - val_loss: 1680270720.0000\n",
            "Epoch 122/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2056004224.0000 - val_loss: 1685100672.0000\n",
            "Epoch 123/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2063675264.0000 - val_loss: 1680212224.0000\n",
            "Epoch 124/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2063702272.0000 - val_loss: 1681421568.0000\n",
            "Epoch 125/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2054486016.0000 - val_loss: 1683711360.0000\n",
            "Epoch 126/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2060216832.0000 - val_loss: 1681980416.0000\n",
            "Epoch 127/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2068982144.0000 - val_loss: 1679678080.0000\n",
            "Epoch 128/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2058590592.0000 - val_loss: 1686067072.0000\n",
            "Epoch 129/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2056717824.0000 - val_loss: 1679356288.0000\n",
            "Epoch 130/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2062579840.0000 - val_loss: 1679496704.0000\n",
            "Epoch 131/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2055633280.0000 - val_loss: 1680260096.0000\n",
            "Epoch 132/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2059880960.0000 - val_loss: 1680876032.0000\n",
            "Epoch 133/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2059010176.0000 - val_loss: 1679063296.0000\n",
            "Epoch 134/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2058266368.0000 - val_loss: 1679227904.0000\n",
            "Epoch 135/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2055871104.0000 - val_loss: 1679796992.0000\n",
            "Epoch 136/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2056140672.0000 - val_loss: 1680434688.0000\n",
            "Epoch 137/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2055623296.0000 - val_loss: 1679029504.0000\n",
            "Epoch 138/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2067618560.0000 - val_loss: 1684860032.0000\n",
            "Epoch 139/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2053209600.0000 - val_loss: 1678753664.0000\n",
            "Epoch 140/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2061579904.0000 - val_loss: 1678274816.0000\n",
            "Epoch 141/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2064567296.0000 - val_loss: 1678831872.0000\n",
            "Epoch 142/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2061934080.0000 - val_loss: 1678776064.0000\n",
            "Epoch 143/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2057851520.0000 - val_loss: 1682252416.0000\n",
            "Epoch 144/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2064193024.0000 - val_loss: 1679475200.0000\n",
            "Epoch 145/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2068144000.0000 - val_loss: 1678017920.0000\n",
            "Epoch 146/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2056421632.0000 - val_loss: 1677772160.0000\n",
            "Epoch 147/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2057438080.0000 - val_loss: 1679141376.0000\n",
            "Epoch 148/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2054736000.0000 - val_loss: 1677889152.0000\n",
            "Epoch 149/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2055680000.0000 - val_loss: 1677804928.0000\n",
            "Epoch 150/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2061558528.0000 - val_loss: 1677884288.0000\n",
            "Epoch 151/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2057507200.0000 - val_loss: 1677236096.0000\n",
            "Epoch 152/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2068809600.0000 - val_loss: 1677162752.0000\n",
            "Epoch 153/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2066173696.0000 - val_loss: 1680960384.0000\n",
            "Epoch 154/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2060535680.0000 - val_loss: 1680242688.0000\n",
            "Epoch 155/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2059125760.0000 - val_loss: 1678826752.0000\n",
            "Epoch 156/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2082690048.0000 - val_loss: 1677537920.0000\n",
            "Epoch 157/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2065616512.0000 - val_loss: 1681126656.0000\n",
            "Epoch 158/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052315008.0000 - val_loss: 1681373440.0000\n",
            "Epoch 159/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2055853568.0000 - val_loss: 1676639872.0000\n",
            "Epoch 160/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2059722112.0000 - val_loss: 1685046656.0000\n",
            "Epoch 161/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2057886336.0000 - val_loss: 1676841856.0000\n",
            "Epoch 162/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2056530176.0000 - val_loss: 1676572160.0000\n",
            "Epoch 163/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2054728576.0000 - val_loss: 1677508352.0000\n",
            "Epoch 164/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2051391872.0000 - val_loss: 1677294592.0000\n",
            "Epoch 165/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2055062272.0000 - val_loss: 1679152000.0000\n",
            "Epoch 166/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 2074611456.0000 - val_loss: 1676010880.0000\n",
            "Epoch 167/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2055542656.0000 - val_loss: 1676615168.0000\n",
            "Epoch 168/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2057115904.0000 - val_loss: 1683150080.0000\n",
            "Epoch 169/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2063905152.0000 - val_loss: 1677742976.0000\n",
            "Epoch 170/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2055782784.0000 - val_loss: 1676539648.0000\n",
            "Epoch 171/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2052449024.0000 - val_loss: 1676231680.0000\n",
            "Epoch 172/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2050220160.0000 - val_loss: 1677404160.0000\n",
            "Epoch 173/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2055919232.0000 - val_loss: 1675813632.0000\n",
            "Epoch 174/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2054973312.0000 - val_loss: 1675739520.0000\n",
            "Epoch 175/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2055455744.0000 - val_loss: 1676386176.0000\n",
            "Epoch 176/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2052767104.0000 - val_loss: 1676356736.0000\n",
            "Epoch 177/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2050975872.0000 - val_loss: 1675967616.0000\n",
            "Epoch 178/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2052985856.0000 - val_loss: 1675416576.0000\n",
            "Epoch 179/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2059234304.0000 - val_loss: 1680355968.0000\n",
            "Epoch 180/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2050762624.0000 - val_loss: 1675566592.0000\n",
            "Epoch 181/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2058467456.0000 - val_loss: 1675191808.0000\n",
            "Epoch 182/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2056672384.0000 - val_loss: 1674864000.0000\n",
            "Epoch 183/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2053681152.0000 - val_loss: 1676175872.0000\n",
            "Epoch 184/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2058292480.0000 - val_loss: 1677360640.0000\n",
            "Epoch 185/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2050975488.0000 - val_loss: 1675776896.0000\n",
            "Epoch 186/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2050299904.0000 - val_loss: 1676046592.0000\n",
            "Epoch 187/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2057414144.0000 - val_loss: 1674151680.0000\n",
            "Epoch 188/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2059002880.0000 - val_loss: 1675000064.0000\n",
            "Epoch 189/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2055363968.0000 - val_loss: 1678611712.0000\n",
            "Epoch 190/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2055519616.0000 - val_loss: 1674809728.0000\n",
            "Epoch 191/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2051482368.0000 - val_loss: 1673887616.0000\n",
            "Epoch 192/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2049593088.0000 - val_loss: 1673665792.0000\n",
            "Epoch 193/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2056389632.0000 - val_loss: 1674169472.0000\n",
            "Epoch 194/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2061329664.0000 - val_loss: 1680900864.0000\n",
            "Epoch 195/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2047996544.0000 - val_loss: 1673840000.0000\n",
            "Epoch 196/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052707072.0000 - val_loss: 1674143232.0000\n",
            "Epoch 197/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2057137920.0000 - val_loss: 1676556032.0000\n",
            "Epoch 198/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2050935680.0000 - val_loss: 1674282752.0000\n",
            "Epoch 199/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2049438976.0000 - val_loss: 1673150336.0000\n",
            "Epoch 200/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2049911936.0000 - val_loss: 1677334656.0000\n",
            "Epoch 201/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2049369728.0000 - val_loss: 1673787776.0000\n",
            "Epoch 202/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2053960192.0000 - val_loss: 1672840832.0000\n",
            "Epoch 203/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2055621888.0000 - val_loss: 1672646912.0000\n",
            "Epoch 204/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2054318336.0000 - val_loss: 1672909312.0000\n",
            "Epoch 205/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2046735104.0000 - val_loss: 1672693504.0000\n",
            "Epoch 206/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2049447424.0000 - val_loss: 1675274112.0000\n",
            "Epoch 207/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2051886464.0000 - val_loss: 1673425536.0000\n",
            "Epoch 208/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2057043712.0000 - val_loss: 1678618112.0000\n",
            "Epoch 209/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052254336.0000 - val_loss: 1675752832.0000\n",
            "Epoch 210/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2052422656.0000 - val_loss: 1675814784.0000\n",
            "Epoch 211/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2050978432.0000 - val_loss: 1672061952.0000\n",
            "Epoch 212/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2047540480.0000 - val_loss: 1673500416.0000\n",
            "Epoch 213/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2074376192.0000 - val_loss: 1672788608.0000\n",
            "Epoch 214/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2048867968.0000 - val_loss: 1672505600.0000\n",
            "Epoch 215/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2051986176.0000 - val_loss: 1672740224.0000\n",
            "Epoch 216/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2044987904.0000 - val_loss: 1672939136.0000\n",
            "Epoch 217/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052100224.0000 - val_loss: 1674827520.0000\n",
            "Epoch 218/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2050985984.0000 - val_loss: 1672063872.0000\n",
            "Epoch 219/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2047599616.0000 - val_loss: 1671702656.0000\n",
            "Epoch 220/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2054656896.0000 - val_loss: 1672044800.0000\n",
            "Epoch 221/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2056522496.0000 - val_loss: 1671924864.0000\n",
            "Epoch 222/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2046836352.0000 - val_loss: 1671593472.0000\n",
            "Epoch 223/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2046869376.0000 - val_loss: 1674744448.0000\n",
            "Epoch 224/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2055608960.0000 - val_loss: 1672059520.0000\n",
            "Epoch 225/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2086717312.0000 - val_loss: 1671880064.0000\n",
            "Epoch 226/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2062095744.0000 - val_loss: 1670875392.0000\n",
            "Epoch 227/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2045497216.0000 - val_loss: 1672690432.0000\n",
            "Epoch 228/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2058057216.0000 - val_loss: 1671207552.0000\n",
            "Epoch 229/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2055531520.0000 - val_loss: 1678899712.0000\n",
            "Epoch 230/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2068246784.0000 - val_loss: 1676723584.0000\n",
            "Epoch 231/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2048688384.0000 - val_loss: 1672445696.0000\n",
            "Epoch 232/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052723584.0000 - val_loss: 1672724992.0000\n",
            "Epoch 233/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2045080704.0000 - val_loss: 1671585280.0000\n",
            "Epoch 234/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2044042496.0000 - val_loss: 1670821120.0000\n",
            "Epoch 235/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2045439616.0000 - val_loss: 1671052160.0000\n",
            "Epoch 236/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2061946624.0000 - val_loss: 1671129728.0000\n",
            "Epoch 237/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2048918656.0000 - val_loss: 1670736128.0000\n",
            "Epoch 238/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2043067264.0000 - val_loss: 1672675328.0000\n",
            "Epoch 239/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2043778432.0000 - val_loss: 1674857984.0000\n",
            "Epoch 240/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2042414208.0000 - val_loss: 1672193664.0000\n",
            "Epoch 241/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2043321984.0000 - val_loss: 1671172992.0000\n",
            "Epoch 242/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052367872.0000 - val_loss: 1673601664.0000\n",
            "Epoch 243/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2038588416.0000 - val_loss: 1673701632.0000\n",
            "Epoch 244/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2044466048.0000 - val_loss: 1671032064.0000\n",
            "Epoch 245/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2049244672.0000 - val_loss: 1670768768.0000\n",
            "Epoch 246/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2040504192.0000 - val_loss: 1670787712.0000\n",
            "Epoch 247/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2044618368.0000 - val_loss: 1669957120.0000\n",
            "Epoch 248/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2039898880.0000 - val_loss: 1671343104.0000\n",
            "Epoch 249/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2042697472.0000 - val_loss: 1673915264.0000\n",
            "Epoch 250/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2045795712.0000 - val_loss: 1671498880.0000\n",
            "Epoch 251/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 2046321024.0000 - val_loss: 1669744000.0000\n",
            "Epoch 252/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2038052736.0000 - val_loss: 1672103680.0000\n",
            "Epoch 253/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2041387392.0000 - val_loss: 1671738112.0000\n",
            "Epoch 254/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2052770688.0000 - val_loss: 1670432896.0000\n",
            "Epoch 255/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2039096704.0000 - val_loss: 1673053184.0000\n",
            "Epoch 256/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2049742848.0000 - val_loss: 1669971968.0000\n",
            "Epoch 257/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2050753664.0000 - val_loss: 1670414976.0000\n",
            "Epoch 258/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2046996352.0000 - val_loss: 1670330752.0000\n",
            "Epoch 259/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2042491648.0000 - val_loss: 1670238592.0000\n",
            "Epoch 260/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2043884160.0000 - val_loss: 1669944704.0000\n",
            "Epoch 261/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2042994176.0000 - val_loss: 1672121472.0000\n",
            "Epoch 262/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2038126080.0000 - val_loss: 1670423552.0000\n",
            "Epoch 263/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2036718336.0000 - val_loss: 1669023616.0000\n",
            "Epoch 264/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2041779072.0000 - val_loss: 1668608384.0000\n",
            "Epoch 265/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2042659712.0000 - val_loss: 1668419968.0000\n",
            "Epoch 266/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2044195712.0000 - val_loss: 1668853376.0000\n",
            "Epoch 267/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2040498944.0000 - val_loss: 1668475392.0000\n",
            "Epoch 268/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 2039490816.0000 - val_loss: 1668074240.0000\n",
            "Epoch 269/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2039876864.0000 - val_loss: 1667732992.0000\n",
            "Epoch 270/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2041558784.0000 - val_loss: 1670581120.0000\n",
            "Epoch 271/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2033956608.0000 - val_loss: 1668229504.0000\n",
            "Epoch 272/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2046758272.0000 - val_loss: 1668596224.0000\n",
            "Epoch 273/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2040980736.0000 - val_loss: 1668249984.0000\n",
            "Epoch 274/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2033992704.0000 - val_loss: 1670307328.0000\n",
            "Epoch 275/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2033767552.0000 - val_loss: 1669141376.0000\n",
            "Epoch 276/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2041029504.0000 - val_loss: 1668626816.0000\n",
            "Epoch 277/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2041328512.0000 - val_loss: 1667942656.0000\n",
            "Epoch 278/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2059747968.0000 - val_loss: 1668806016.0000\n",
            "Epoch 279/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2038101248.0000 - val_loss: 1667803264.0000\n",
            "Epoch 280/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2039007488.0000 - val_loss: 1668460544.0000\n",
            "Epoch 281/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2036436480.0000 - val_loss: 1666581120.0000\n",
            "Epoch 282/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2034685824.0000 - val_loss: 1670323712.0000\n",
            "Epoch 283/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2038179968.0000 - val_loss: 1667271936.0000\n",
            "Epoch 284/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2033820032.0000 - val_loss: 1667630464.0000\n",
            "Epoch 285/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2032909184.0000 - val_loss: 1667243520.0000\n",
            "Epoch 286/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2048023680.0000 - val_loss: 1670294656.0000\n",
            "Epoch 287/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2034542336.0000 - val_loss: 1667790464.0000\n",
            "Epoch 288/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2039208704.0000 - val_loss: 1666273920.0000\n",
            "Epoch 289/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2040414720.0000 - val_loss: 1670700160.0000\n",
            "Epoch 290/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2033885568.0000 - val_loss: 1666367104.0000\n",
            "Epoch 291/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2041347968.0000 - val_loss: 1667005696.0000\n",
            "Epoch 292/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2043391232.0000 - val_loss: 1665940480.0000\n",
            "Epoch 293/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2046805888.0000 - val_loss: 1666428672.0000\n",
            "Epoch 294/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2030192768.0000 - val_loss: 1668062080.0000\n",
            "Epoch 295/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2040079360.0000 - val_loss: 1666077952.0000\n",
            "Epoch 296/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2039057024.0000 - val_loss: 1666976896.0000\n",
            "Epoch 297/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2032163456.0000 - val_loss: 1664901632.0000\n",
            "Epoch 298/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2034258688.0000 - val_loss: 1671593216.0000\n",
            "Epoch 299/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2031185280.0000 - val_loss: 1665468032.0000\n",
            "Epoch 300/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2031697408.0000 - val_loss: 1666640768.0000\n",
            "Epoch 301/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2031560448.0000 - val_loss: 1666282240.0000\n",
            "Epoch 302/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2037982464.0000 - val_loss: 1670331904.0000\n",
            "Epoch 303/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2026696960.0000 - val_loss: 1664909312.0000\n",
            "Epoch 304/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2035932416.0000 - val_loss: 1666636288.0000\n",
            "Epoch 305/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2028925696.0000 - val_loss: 1665005952.0000\n",
            "Epoch 306/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2036360960.0000 - val_loss: 1664584960.0000\n",
            "Epoch 307/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2027407872.0000 - val_loss: 1665025920.0000\n",
            "Epoch 308/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2044910080.0000 - val_loss: 1666440320.0000\n",
            "Epoch 309/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2036703104.0000 - val_loss: 1665314048.0000\n",
            "Epoch 310/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2034008064.0000 - val_loss: 1664095360.0000\n",
            "Epoch 311/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2035590144.0000 - val_loss: 1664876672.0000\n",
            "Epoch 312/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2037460096.0000 - val_loss: 1667892608.0000\n",
            "Epoch 313/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2035503872.0000 - val_loss: 1663564544.0000\n",
            "Epoch 314/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2025457920.0000 - val_loss: 1664029696.0000\n",
            "Epoch 315/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2033626880.0000 - val_loss: 1663969408.0000\n",
            "Epoch 316/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2025149824.0000 - val_loss: 1670943360.0000\n",
            "Epoch 317/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2048826240.0000 - val_loss: 1664955392.0000\n",
            "Epoch 318/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2027207552.0000 - val_loss: 1665587968.0000\n",
            "Epoch 319/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2025780480.0000 - val_loss: 1663441280.0000\n",
            "Epoch 320/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2027808512.0000 - val_loss: 1663023104.0000\n",
            "Epoch 321/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2022624128.0000 - val_loss: 1664012032.0000\n",
            "Epoch 322/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2028222080.0000 - val_loss: 1666517504.0000\n",
            "Epoch 323/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2026733696.0000 - val_loss: 1663695488.0000\n",
            "Epoch 324/2000\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 2032401792.0000 - val_loss: 1667743488.0000\n",
            "Epoch 325/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2023482624.0000 - val_loss: 1662417152.0000\n",
            "Epoch 326/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2026193920.0000 - val_loss: 1662644480.0000\n",
            "Epoch 327/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2022850432.0000 - val_loss: 1663471360.0000\n",
            "Epoch 328/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2027849472.0000 - val_loss: 1662025216.0000\n",
            "Epoch 329/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2017443456.0000 - val_loss: 1663844352.0000\n",
            "Epoch 330/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2040370432.0000 - val_loss: 1664223488.0000\n",
            "Epoch 331/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2030213504.0000 - val_loss: 1664730368.0000\n",
            "Epoch 332/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2030807424.0000 - val_loss: 1664457088.0000\n",
            "Epoch 333/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2021009664.0000 - val_loss: 1662347776.0000\n",
            "Epoch 334/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2020854144.0000 - val_loss: 1662160640.0000\n",
            "Epoch 335/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2034758272.0000 - val_loss: 1664746880.0000\n",
            "Epoch 336/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2029356416.0000 - val_loss: 1660434816.0000\n",
            "Epoch 337/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2035446528.0000 - val_loss: 1661519744.0000\n",
            "Epoch 338/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2033935744.0000 - val_loss: 1661268864.0000\n",
            "Epoch 339/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2022811776.0000 - val_loss: 1661205120.0000\n",
            "Epoch 340/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2027432576.0000 - val_loss: 1660355968.0000\n",
            "Epoch 341/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2023906816.0000 - val_loss: 1660030720.0000\n",
            "Epoch 342/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2038104704.0000 - val_loss: 1667640576.0000\n",
            "Epoch 343/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2018147712.0000 - val_loss: 1663268480.0000\n",
            "Epoch 344/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2017420544.0000 - val_loss: 1659893248.0000\n",
            "Epoch 345/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2014122496.0000 - val_loss: 1662781568.0000\n",
            "Epoch 346/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2023984896.0000 - val_loss: 1661262336.0000\n",
            "Epoch 347/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2018300544.0000 - val_loss: 1659765248.0000\n",
            "Epoch 348/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2022420352.0000 - val_loss: 1659244160.0000\n",
            "Epoch 349/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2014770432.0000 - val_loss: 1661394944.0000\n",
            "Epoch 350/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2016164864.0000 - val_loss: 1664141440.0000\n",
            "Epoch 351/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2010510592.0000 - val_loss: 1659750784.0000\n",
            "Epoch 352/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2015358464.0000 - val_loss: 1658531328.0000\n",
            "Epoch 353/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2012485120.0000 - val_loss: 1659770496.0000\n",
            "Epoch 354/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2024650624.0000 - val_loss: 1660086528.0000\n",
            "Epoch 355/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2015233536.0000 - val_loss: 1659216000.0000\n",
            "Epoch 356/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2012275968.0000 - val_loss: 1672538240.0000\n",
            "Epoch 357/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2014273024.0000 - val_loss: 1659573120.0000\n",
            "Epoch 358/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 2009490432.0000 - val_loss: 1660545664.0000\n",
            "Epoch 359/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2017948544.0000 - val_loss: 1659800576.0000\n",
            "Epoch 360/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2014651904.0000 - val_loss: 1660066816.0000\n",
            "Epoch 361/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2010620544.0000 - val_loss: 1662295552.0000\n",
            "Epoch 362/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2008139392.0000 - val_loss: 1659356416.0000\n",
            "Epoch 363/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2025807872.0000 - val_loss: 1658936320.0000\n",
            "Epoch 364/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2023178112.0000 - val_loss: 1659535872.0000\n",
            "Epoch 365/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2007038592.0000 - val_loss: 1657641088.0000\n",
            "Epoch 366/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2007100160.0000 - val_loss: 1658636288.0000\n",
            "Epoch 367/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2008373632.0000 - val_loss: 1657344000.0000\n",
            "Epoch 368/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2009624832.0000 - val_loss: 1658889472.0000\n",
            "Epoch 369/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2007837696.0000 - val_loss: 1663379712.0000\n",
            "Epoch 370/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2015061760.0000 - val_loss: 1660390784.0000\n",
            "Epoch 371/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2006892032.0000 - val_loss: 1657429888.0000\n",
            "Epoch 372/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2003264256.0000 - val_loss: 1657157888.0000\n",
            "Epoch 373/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2007946112.0000 - val_loss: 1661920896.0000\n",
            "Epoch 374/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2011315968.0000 - val_loss: 1657972992.0000\n",
            "Epoch 375/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2022745856.0000 - val_loss: 1657566208.0000\n",
            "Epoch 376/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2005408000.0000 - val_loss: 1658440832.0000\n",
            "Epoch 377/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2009826048.0000 - val_loss: 1660724224.0000\n",
            "Epoch 378/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2009760384.0000 - val_loss: 1656150528.0000\n",
            "Epoch 379/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2000839808.0000 - val_loss: 1656913664.0000\n",
            "Epoch 380/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2001039104.0000 - val_loss: 1658028160.0000\n",
            "Epoch 381/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1999304448.0000 - val_loss: 1658022400.0000\n",
            "Epoch 382/2000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 2008477312.0000 - val_loss: 1655438976.0000\n",
            "Epoch 383/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2003844480.0000 - val_loss: 1657133696.0000\n",
            "Epoch 384/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1996824448.0000 - val_loss: 1655284224.0000\n",
            "Epoch 385/2000\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 2005937664.0000 - val_loss: 1656391168.0000\n",
            "Epoch 386/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 2003073152.0000 - val_loss: 1657260288.0000\n",
            "Epoch 387/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2002456064.0000 - val_loss: 1661488256.0000\n",
            "Epoch 388/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1999751680.0000 - val_loss: 1655839872.0000\n",
            "Epoch 389/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 2000188032.0000 - val_loss: 1658132480.0000\n",
            "Epoch 390/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 2009621760.0000 - val_loss: 1655559040.0000\n",
            "Epoch 391/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1996495744.0000 - val_loss: 1656339968.0000\n",
            "Epoch 392/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1997652096.0000 - val_loss: 1653722624.0000\n",
            "Epoch 393/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1992774400.0000 - val_loss: 1658614016.0000\n",
            "Epoch 394/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1997102592.0000 - val_loss: 1655429632.0000\n",
            "Epoch 395/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 2004591616.0000 - val_loss: 1655721344.0000\n",
            "Epoch 396/2000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 2007165440.0000 - val_loss: 1655089792.0000\n",
            "Epoch 397/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1994520576.0000 - val_loss: 1655832192.0000\n",
            "Epoch 398/2000\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 1995817216.0000 - val_loss: 1655217024.0000\n",
            "Epoch 399/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2004912640.0000 - val_loss: 1655013504.0000\n",
            "Epoch 400/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1990128128.0000 - val_loss: 1655860992.0000\n",
            "Epoch 401/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1994897792.0000 - val_loss: 1655842432.0000\n",
            "Epoch 402/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1991506560.0000 - val_loss: 1653772160.0000\n",
            "Epoch 403/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1989394944.0000 - val_loss: 1652906624.0000\n",
            "Epoch 404/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1999124352.0000 - val_loss: 1652586112.0000\n",
            "Epoch 405/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1988717056.0000 - val_loss: 1655951872.0000\n",
            "Epoch 406/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1996919168.0000 - val_loss: 1652119168.0000\n",
            "Epoch 407/2000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 1988535680.0000 - val_loss: 1655852032.0000\n",
            "Epoch 408/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1987256832.0000 - val_loss: 1652465152.0000\n",
            "Epoch 409/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1991480832.0000 - val_loss: 1660241152.0000\n",
            "Epoch 410/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 2006693760.0000 - val_loss: 1653975296.0000\n",
            "Epoch 411/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1998189696.0000 - val_loss: 1654440576.0000\n",
            "Epoch 412/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1996380544.0000 - val_loss: 1652020096.0000\n",
            "Epoch 413/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1984244224.0000 - val_loss: 1660921856.0000\n",
            "Epoch 414/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1986280832.0000 - val_loss: 1652163968.0000\n",
            "Epoch 415/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1987014912.0000 - val_loss: 1651966592.0000\n",
            "Epoch 416/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1982601088.0000 - val_loss: 1651716864.0000\n",
            "Epoch 417/2000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 1991609472.0000 - val_loss: 1656430080.0000\n",
            "Epoch 418/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1982001152.0000 - val_loss: 1651804160.0000\n",
            "Epoch 419/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1991868160.0000 - val_loss: 1652006400.0000\n",
            "Epoch 420/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1981182336.0000 - val_loss: 1652829312.0000\n",
            "Epoch 421/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1987885184.0000 - val_loss: 1651868160.0000\n",
            "Epoch 422/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1980317312.0000 - val_loss: 1653520512.0000\n",
            "Epoch 423/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1988432256.0000 - val_loss: 1654829952.0000\n",
            "Epoch 424/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1985288064.0000 - val_loss: 1656653952.0000\n",
            "Epoch 425/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1986985344.0000 - val_loss: 1653338496.0000\n",
            "Epoch 426/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1979506048.0000 - val_loss: 1652704000.0000\n",
            "Epoch 427/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1988248064.0000 - val_loss: 1651584128.0000\n",
            "Epoch 428/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1985038592.0000 - val_loss: 1652636672.0000\n",
            "Epoch 429/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1989131136.0000 - val_loss: 1650450048.0000\n",
            "Epoch 430/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 2001318272.0000 - val_loss: 1660534912.0000\n",
            "Epoch 431/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1978469120.0000 - val_loss: 1649782144.0000\n",
            "Epoch 432/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1975492608.0000 - val_loss: 1651745408.0000\n",
            "Epoch 433/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1975249536.0000 - val_loss: 1650441088.0000\n",
            "Epoch 434/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1984993280.0000 - val_loss: 1650823168.0000\n",
            "Epoch 435/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1976284032.0000 - val_loss: 1652334592.0000\n",
            "Epoch 436/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1977583104.0000 - val_loss: 1649892224.0000\n",
            "Epoch 437/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1974777344.0000 - val_loss: 1650340736.0000\n",
            "Epoch 438/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1971412608.0000 - val_loss: 1652895104.0000\n",
            "Epoch 439/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1969396096.0000 - val_loss: 1649719936.0000\n",
            "Epoch 440/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1978027392.0000 - val_loss: 1650138624.0000\n",
            "Epoch 441/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1975533184.0000 - val_loss: 1651193088.0000\n",
            "Epoch 442/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1973076224.0000 - val_loss: 1652005760.0000\n",
            "Epoch 443/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1977152896.0000 - val_loss: 1650335616.0000\n",
            "Epoch 444/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1976512000.0000 - val_loss: 1652299392.0000\n",
            "Epoch 445/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1968646656.0000 - val_loss: 1651109120.0000\n",
            "Epoch 446/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1968820480.0000 - val_loss: 1650208384.0000\n",
            "Epoch 447/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1977624448.0000 - val_loss: 1649708032.0000\n",
            "Epoch 448/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1972202880.0000 - val_loss: 1650260224.0000\n",
            "Epoch 449/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1968366848.0000 - val_loss: 1654859648.0000\n",
            "Epoch 450/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1972128512.0000 - val_loss: 1649656192.0000\n",
            "Epoch 451/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1968713984.0000 - val_loss: 1651349760.0000\n",
            "Epoch 452/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1971425920.0000 - val_loss: 1649223040.0000\n",
            "Epoch 453/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1968884736.0000 - val_loss: 1651295488.0000\n",
            "Epoch 454/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1978027648.0000 - val_loss: 1651716096.0000\n",
            "Epoch 455/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1965714816.0000 - val_loss: 1655857792.0000\n",
            "Epoch 456/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1979525504.0000 - val_loss: 1650887552.0000\n",
            "Epoch 457/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1964080256.0000 - val_loss: 1651723392.0000\n",
            "Epoch 458/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1968833408.0000 - val_loss: 1652151168.0000\n",
            "Epoch 459/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1968493696.0000 - val_loss: 1649876480.0000\n",
            "Epoch 460/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1963209728.0000 - val_loss: 1647968768.0000\n",
            "Epoch 461/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1970259328.0000 - val_loss: 1653587072.0000\n",
            "Epoch 462/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1961540096.0000 - val_loss: 1648609536.0000\n",
            "Epoch 463/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1962467456.0000 - val_loss: 1651265408.0000\n",
            "Epoch 464/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1967256704.0000 - val_loss: 1650711680.0000\n",
            "Epoch 465/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1959632000.0000 - val_loss: 1649576448.0000\n",
            "Epoch 466/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1956049408.0000 - val_loss: 1648632192.0000\n",
            "Epoch 467/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1962749184.0000 - val_loss: 1649126656.0000\n",
            "Epoch 468/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1960802816.0000 - val_loss: 1648075264.0000\n",
            "Epoch 469/2000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 1965446784.0000 - val_loss: 1649700480.0000\n",
            "Epoch 470/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1974612992.0000 - val_loss: 1649075968.0000\n",
            "Epoch 471/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1957353600.0000 - val_loss: 1660241152.0000\n",
            "Epoch 472/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1954740224.0000 - val_loss: 1647781632.0000\n",
            "Epoch 473/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1974727296.0000 - val_loss: 1647139328.0000\n",
            "Epoch 474/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1965154176.0000 - val_loss: 1652084480.0000\n",
            "Epoch 475/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1967811712.0000 - val_loss: 1650114688.0000\n",
            "Epoch 476/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1952670464.0000 - val_loss: 1647790080.0000\n",
            "Epoch 477/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1955557248.0000 - val_loss: 1647900416.0000\n",
            "Epoch 478/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1960096000.0000 - val_loss: 1647264768.0000\n",
            "Epoch 479/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1957398528.0000 - val_loss: 1647195776.0000\n",
            "Epoch 480/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1957003776.0000 - val_loss: 1647577600.0000\n",
            "Epoch 481/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1955174144.0000 - val_loss: 1647965056.0000\n",
            "Epoch 482/2000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 1953170816.0000 - val_loss: 1647640192.0000\n",
            "Epoch 483/2000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 1951790208.0000 - val_loss: 1647444480.0000\n",
            "Epoch 484/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1955949056.0000 - val_loss: 1648029568.0000\n",
            "Epoch 485/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1956346240.0000 - val_loss: 1651893632.0000\n",
            "Epoch 486/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1948092672.0000 - val_loss: 1648668416.0000\n",
            "Epoch 487/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1946370560.0000 - val_loss: 1656689536.0000\n",
            "Epoch 488/2000\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 1949863936.0000 - val_loss: 1650004224.0000\n",
            "Epoch 489/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1946143488.0000 - val_loss: 1648119040.0000\n",
            "Epoch 490/2000\n",
            "28/28 [==============================] - 0s 6ms/step - loss: 1951644416.0000 - val_loss: 1648978816.0000\n",
            "Epoch 491/2000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 1963222912.0000 - val_loss: 1647666816.0000\n",
            "Epoch 492/2000\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 1957161984.0000 - val_loss: 1662895360.0000\n",
            "Epoch 493/2000\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 1954377600.0000 - val_loss: 1649176576.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손글씨 숫자 인식하기\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0],28,28,1).astype('float32')/255\n",
        "X_test = X_test.reshape(X_test.shape[0],28,28,1).astype('float32')/255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelpath = './MNIST_CNN.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verboss=1, save_best_only=True)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])\n",
        "\n",
        "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n",
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_loss))\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC4s_J9a_Fm9",
        "outputId": "4eb51c67-a41b-4003-bd68-3b4c896d8bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0352 - accuracy: 0.9914\n",
            "\n",
            " Test Accuracy: 0.9914\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNQt99B0R4M9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}